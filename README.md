# Query Fit or CSV data using Natural Language. Add a relevant research paper including formulas and reasoning to add context to the query.
This repository contains a Python scripts that utilize OpenAI's GPT-powered API to query and generate answers for a given prompt in natural language. The script exposes Natural Language Query Endpoint: **/nat_lang_query** endpoint that allows users to generate answers using OpenAI's API. 

It takes *prompt, doc_path, table, activity_id, and model(Optional)* as inputs, and retrieves the relevant data from the specified table (csv_data or fit_data). The endpoint then creates a pandas DataFrame containing the data, cleans it, and loads the research document specified by doc_path using the UnstructuredPDFLoader. The document is split into chunks and passed to the Chroma vector database, which is then used to create a RetrievalQA instance. 

The endpoint combines the DataFrame and the document search tool to create a ZeroShotAgent, which is then used to generate the response based on the input prompt. This endpoint provides an efficient way to leverage natural language processing to analyze and extract information from both structured (pandas DataFrames) and unstructured data (documents).

The package is delivered as a FastAPI web app and requires an OpenAI API key, and a PostgreSQL database to function. I am using a free tier Supabase but a local instance can also be used. The OpenAI API key needs to be stored in a 'OPENAI_API_KEY' environment variable or in a "openai_api_key" variable (not secure) and the database connection string is stored in plain text in the database.py module (that is lame Palo :-)).
The required python libraries are listed in the requirements.txt file generated by pip freeze and is a bit of a mess and needs a cleanup :-).

The script will install the DB schema into an existing blank database upon the first execution. Subsequently the user will need to be creaded using FastAPI **/signup** endpoint and loged in using the **/login** endpoint. The fit file is supplied for parsing and data storage using **/fit_activities** endpoint. A bulk upload is also available via **/bulk_upload_fit** endpoint suplying a zip file containing multiple fit files. A sample fit files to play around with are included in the repo.


Lot of work to be done yet, but the initial results are encouraging :-)

**Usage:**

Start the app: *uvicorn main:app --host 0.0.0.0 --port 8000 --reload*

FastAPI URL: *http://127.0.0.1/docs

**Sample outputs:**

*Prompt: Get the formula for the HR-Running-Speed-Index, then plug in the mean hr and mean speed data from the df, and the standing hr of 56, maximal hr of 182, and the vo2max running speed of 3.5 m/s. Calculate the results and return the HR-Running-Speed-Index value in numeric format together with the formula used.*
![](images/output_6.png)

*Prompt: What 3 columns correlate most with the skin_temperature column ? Provide Pearson coefficient in the response.*
![](images/output_1.png)

*Prompt: At what time did the highest core temperature occured and for how long ?*
![](images/output_2.png)

*Prompt: Print out standard deviation for all columns, and order from highest to lowest*
![](images/output_3.png)

*Prompt: What is the average running speed in km/h and total duration of activity in minutes ?*
![](images/output_4.png)

*FastAPI endpoint parameters and the corresponding response example*
![](images/output_5.png)
